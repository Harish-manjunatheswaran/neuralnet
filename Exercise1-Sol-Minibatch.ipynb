{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "In this exercise you will program a simple neural network with numpy. The tasks will be:\n",
    "1. Implementing a Forward propagation\n",
    "2. Completing the Forward propagation by computing the loss\n",
    "3. Implementing a Back propagation\n",
    "4. Training the neural network\n",
    "\n",
    "You will implement code in 'nn.py' and test your implementations with the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Classifier\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, std=1e-4):\n",
    "       \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_dim, hidden_dim)\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_dim, output_dim)\n",
    "        self.params['b2'] = np.zeros(output_dim)\n",
    "        \n",
    "    def loss_grad(self, X, y=None, reg=0.0):\n",
    "        \n",
    "        # Unpack variables from the params dictionary\n",
    "    \n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        W2 = self.params['W2'] \n",
    "        b2 = self.params['b2']\n",
    "        N, D = X.shape\n",
    "       \n",
    "        #--------------------------------------- forward propagation --------------------------------------- \n",
    "        \n",
    "        scores = None\n",
    "        loops = True\n",
    "        \n",
    "        if loops == False:\n",
    "            \n",
    "            # Task 1.1:\n",
    "            \n",
    "            # Compute the class scores for the input.\n",
    "            # Use the weights and biases to perform a forward propagation and store the results\n",
    "            # in the scores variable, which should be an array of shape (N, C).\n",
    "            # Start with a naive implementation with at least 2 loops.\n",
    "            \n",
    "            pro_1 = [[ 0 for i in range(len(W1[0]))] for j in range(len(X))]\n",
    "\n",
    "            # Insert a for loop for matrix product of X & W1\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                # iterate through columns of Y\n",
    "                for j in range(len(W1[0])):\n",
    "                    #iterate through rows of Y\n",
    "                    for k in range(len(W1)):\n",
    "                        pro_1[i][j] += X[i][k] * W1[k][j]\n",
    "\n",
    "            # Find the sum z2\n",
    "\n",
    "            z2 = pro_1 + b1\n",
    "\n",
    "            # Implement ReLu\n",
    "            \n",
    "            a2 = np.maximum(0, z2)\n",
    "\n",
    "            # Insert a for loop for matrix product of a2 & w2\n",
    "\n",
    "            pro_2 = [[ 0 for i in range(len(W2[0]))] for j in range(len(a2))]\n",
    "\n",
    "            for i in range(len(a2)):\n",
    "                # iterate through columns of Y\n",
    "                for j in range(len(W2[0])):\n",
    "                    #iterate through rows of Y\n",
    "                    for k in range(len(W2)):\n",
    "                        pro_2[i][j] += a2[i][k] * W2[k][j]\n",
    "\n",
    "            scores = pro_2 + b2\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Task 1.2:\n",
    "            \n",
    "            # Now implement the same forward propagation as you did above using no loops.\n",
    "            # If you are done set the parameter loops to False to test your code. \n",
    "        \n",
    "            z2 = np.dot(X, W1) + b1\n",
    "            a2 = np.maximum(0, z2)\n",
    "            scores = np.dot(a2, W2) + b2\n",
    "\n",
    "        \n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        #--------------------------------------- loss function ---------------------------------------------\n",
    "\n",
    "        loss = None\n",
    "        \n",
    "        # Task 2:\n",
    "        \n",
    "        # Compute the loss with softmax and store it in the variable loss. Include L2 regularization for W1 and W2.\n",
    "        # Make sure to handle numerical instabilities.\n",
    "        \n",
    "        # Softmax \n",
    "        exp_scores = np.exp(scores - np.amax(scores)) \n",
    "        probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True)\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        log_probs = -np.log(probs[range(N), y])\n",
    "        cross_entropy_loss = np.sum(log_probs) / N\n",
    "        \n",
    "        # Regularization\n",
    "        regularizer = ((reg * np.sum(np.square(W1))) + (reg * np.sum(np.square(W2))))\n",
    "        \n",
    "        # Loss function\n",
    "        loss = cross_entropy_loss + regularizer\n",
    "        \n",
    "        #--------------------------------------- back propagation -------------------------------------------\n",
    "        \n",
    "        grads = {}   \n",
    "        \n",
    "        # Task 3: \n",
    "        \n",
    "        # Compute the derivatives of the weights and biases (back propagation).\n",
    "        # Store the results in the grads dictionary, where 'W1' referes to the gradient of W1 etc.\n",
    "        \n",
    "        def reluDerivative(r):\n",
    "            r[r<=0] = 0\n",
    "            r[r>0] = 1\n",
    "            return r\n",
    "        \n",
    "        dw_scores = probs\n",
    "        dw_scores[range(N), y] -= 1\n",
    "        dw_scores = dw_scores / N\n",
    "        \n",
    "        # dLoss/dW2\n",
    "        dW_2 = np.dot(a2.T, dw_scores) + (2 * reg * W2)\n",
    "        \n",
    "        # dLoss/db2\n",
    "        db_2 = np.sum(dw_scores, axis = 0, keepdims = True)\n",
    "        \n",
    "        # dLoss/dW1\n",
    "        dW_1 = np.dot(X.T, (np.dot(dw_scores, W2.T)) * reluDerivative(a2)) + (2 * reg * W1)\n",
    "        \n",
    "        # dLoss/db1\n",
    "        db_1 = np.sum(np.dot(dw_scores, W2.T) * reluDerivative(a2), axis = 0, keepdims = True)\n",
    "\n",
    "        grads.update({\"W1\": dW_1, \"W2\": dW_2, \"b1\": db_1, \"b2\": db_2})\n",
    "        return loss, grads\n",
    "\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val,\n",
    "          learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "          reg=5e-6, num_iters=100,\n",
    "          batch_size=200, verbose=False):\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                \n",
    "        # Task 4.1:\n",
    "        \n",
    "        # Create a random minibatch of training data X and labels y, and stor\n",
    "        # them in X_batch and y_batch.\n",
    "            \n",
    "            minis = []\n",
    "\n",
    "            np.random.seed(0)\n",
    "            num_train = X.shape[0]\n",
    "            rand_indices = np.random.choice(np.arange(num_train), size = 1000, replace = True)\n",
    "            X_b = X[rand_indices, :]\n",
    "            y_b = y[rand_indices]\n",
    "            n_minibatches = X_b.shape[0] // batch_size\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            for i in range(n_minibatches):\n",
    "                mini_X = X_b[i * batch_size:(i + 1)*batch_size, :]\n",
    "                mini_y = y_b[i * batch_size:(i + 1)*batch_size]\n",
    "\n",
    "                X_bat = mini_X[:, :]\n",
    "                y_bat = mini_y[:]\n",
    "                minis.append((X_bat, y_bat))\n",
    "                \n",
    "            mini_batch = minis \n",
    "                           \n",
    "            for mini in mini_batch:\n",
    "                \n",
    "                X_batch, y_batch = mini\n",
    "                \n",
    "                # Compute the Loss & Gradients of current minibatch\n",
    "            \n",
    "                loss, grads = self.loss_grad(X_batch, y=y_batch, reg=reg)\n",
    "                loss_history.append(loss)\n",
    "                \n",
    "            \n",
    "            # Task 4.2:\n",
    "            \n",
    "            # Update the parameters of the network (in self.params) by using stochastic gradient descent. \n",
    "\n",
    "                self.params['W1'] = self.params['W1'] - (learning_rate * grads[\"W1\"])\n",
    "                self.params['W2'] = self.params['W2'] - (learning_rate * grads[\"W2\"])\n",
    "                self.params['b1'] = self.params['b1'] - (learning_rate * grads[\"b1\"])\n",
    "                self.params['b2'] = self.params['b2'] - (learning_rate * grads[\"b2\"])\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "            if verbose and it % 10 == 0:\n",
    "                print('iteration %sd / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                \n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "        \n",
    "        return {\n",
    "\n",
    "            'loss_history': loss_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history\n",
    "\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "    \n",
    "        y_pred = None\n",
    "        \n",
    "        # Task 4.3: \n",
    "        \n",
    "        # Implement this function to predict labels for the data points.\n",
    "        \n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        W2 = self.params['W2'] \n",
    "        b2 = self.params['b2']\n",
    "        \n",
    "        a2 = np.maximum(0, np.dot(X, W1) + b1)\n",
    "        scores = np.dot(a2, W2) + b2\n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code initializes the network (see __init__ in nn.py) and some other functions needed later. \n",
    "# The input data (X) with the associated labels (y) as well as the weights and biases \n",
    "# are initialized with random numbers. A seed is set to make your results comparable.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dim = 4\n",
    "hidden_dim = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_net():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_dim, hidden_dim, num_classes, std=1e-1)\n",
    "\n",
    "def init_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_dim)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "net = init_net()\n",
    "X, y = init_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation\n",
    "Go to 'nn.py' to implement the Forward propagation in the loss_grad function (Task 1.1 and Task 1.2).\n",
    "Then test the code below to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.6802720496109664e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss_grad(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "# The difference should be very small. (< 1e-7)\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time difference\n",
    "Add code to measure the time of your naive implementation with at least 2 loops and the implementation with no loops and fill in your results below. \n",
    "Note that 'time' is already imported in the 'nn.py' file which you can use for the time measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration with 2 loops:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration without loops:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwardpass loss\n",
    "Complete the forward propagation by implementing the loss function in 'nn.py' (Task 2).\n",
    "Run the code below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3037878913298206\n",
      "Difference between your loss and correct loss:\n",
      "1.794120407794253e-13\n"
     ]
    }
   ],
   "source": [
    "loss, _= net.loss_grad(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "print(loss)\n",
    "\n",
    "# should be very small, (< 1e-12)\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical issues with Softmax\n",
    "\n",
    "Please describe the two main issues that can lead to numerical instability when using the softmax function? Describe a/your approach of avoiding instabilities with softmax."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Softmax function is prone to two issues: overflow and underflow\n",
    "\n",
    "Overflow: It occurs when very large numbers are approximated as infinity\n",
    "\n",
    "Underflow: It occurs when very small numbers (near zero in the number line) are approximated (i.e. rounded to) as zero\n",
    "\n",
    "To combat these issues when doing softmax computation, a common trick is to shift the input vector by subtracting the maximum element in it from all elements. For the input vector x, define z such that:\n",
    "\n",
    "z = x- np.amax(x)\n",
    "\n",
    "And then take the softmax of the new and stable vector z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "Now implement the Back propagation in 'nn.py' (Task 3) in the loss_grad function.\n",
    "Test the code below to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 3.561318e-09\n",
      "W2 max relative error: 3.440708e-09\n",
      "b1 max relative error: 2.738421e-09\n",
      "b2 max relative error: 3.865091e-11\n"
     ]
    }
   ],
   "source": [
    "from eval_grad import numerical_grad\n",
    "\n",
    "# Here we use a numeric gradient to check your implementation of the back propagation.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss_grad(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss_grad(X, y, reg=0.05)[0]\n",
    "    param_grad_num = numerical_grad(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network\n",
    "Finaly we want to train our network. For this purpose go to 'nn.py' and implement the train and predict function (Task 4.1, 4.2 and 4.3).\n",
    "Then test the code below to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0d / 100: loss 0.437614\n",
      "iteration 10d / 100: loss 0.016142\n",
      "iteration 20d / 100: loss 0.014579\n",
      "iteration 30d / 100: loss 0.014469\n",
      "iteration 40d / 100: loss 0.014461\n",
      "iteration 50d / 100: loss 0.014460\n",
      "iteration 60d / 100: loss 0.014460\n",
      "iteration 70d / 100: loss 0.014460\n",
      "iteration 80d / 100: loss 0.014460\n",
      "iteration 90d / 100: loss 0.014460\n",
      "Final training loss:  0.014460416251124625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_net()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=True)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations\n",
    "Please explain at least how many iterations are useful for training this neural network (use the plot above for your explanation)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above plot, we can observe that the loss reaches a steady value after roughly 20 iterations, the loss reaches a steady value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
